# =============================================================================
# FLINK STATEMENTS MODULE
# =============================================================================

# =============================================================================
# DATA SOURCES
# =============================================================================

data "confluent_flink_compute_pool" "by_name" {
  for_each = toset(local.all_compute_pools)
  
  display_name = each.value
  
  environment {
    id = var.environment_id
  }
}

data "confluent_flink_region" "by_region" {
  for_each = toset([
    for pool in data.confluent_flink_compute_pool.by_name : pool.region
  ])
  
  cloud  = data.confluent_flink_compute_pool.by_name[keys(data.confluent_flink_compute_pool.by_name)[0]].cloud
  region = each.value
}

# =============================================================================
# LOCAL VALUES
# =============================================================================
locals {
  # Confluent Flink API credentials (for statement management)
  confluent_cloud_api_key    = var.confluent_cloud_api_key
  confluent_cloud_api_secret = var.confluent_cloud_api_secret
  confluent_flink_api_key    = var.confluent_flink_api_key
  confluent_flink_api_secret = var.confluent_flink_api_secret
  
  
  # Cargar archivos YAML de DDL y DML
  ddl_files = fileset("${var.statements_dir}/ddl", "*.yaml")
  ddl_data  = [for f in local.ddl_files : yamldecode(file("${var.statements_dir}/ddl/${f}"))]
  
  dml_files = fileset("${var.statements_dir}/dml", "*.yaml")
  dml_data  = [for f in local.dml_files : yamldecode(file("${var.statements_dir}/dml/${f}"))]
  
  # Extraer compute pools únicos de los archivos YAML
  all_compute_pools = distinct(concat(
    [for ddl in local.ddl_data : ddl["flink-compute-pool"]],
    [for dml in local.dml_data : dml["flink-compute-pool"]]
  ))
  
  # Mapeo de compute pools (necesario para asociar statements con compute pools)
  compute_pools_map = {
    for pool_name in local.all_compute_pools :
    pool_name => {
      id = data.confluent_flink_compute_pool.by_name[pool_name].id
      rest_endpoint = data.confluent_flink_region.by_region[data.confluent_flink_compute_pool.by_name[pool_name].region].rest_endpoint
      # Para clusters dedicados con private networking, usar private_rest_endpoint
      private_rest_endpoint = data.confluent_flink_region.by_region[data.confluent_flink_compute_pool.by_name[pool_name].region].private_rest_endpoint
    }
  }
}

# =============================================================================
# RESOURCES
# =============================================================================

# -----------------------------------------------------------------------------
# DDL Statements (Data Definition Language) - Usando local-exec para evitar rest_endpoint
# -----------------------------------------------------------------------------
resource "null_resource" "ddl_statements" {
  count = length(local.ddl_data)
  
  provisioner "local-exec" {
    command = <<-EOT
      # Configurar credenciales usando archivo de configuración
      mkdir -p ~/.confluent
      cat > ~/.confluent/config << EOF
[api]
api_key = ${trimspace(var.confluent_flink_api_key)}
api_secret = ${trimspace(var.confluent_flink_api_secret)}
url = https://confluent.cloud
EOF

      # Configurar variables de entorno también
      export CONFLUENT_CLOUD_API_KEY="${trimspace(var.confluent_flink_api_key)}"
      export CONFLUENT_CLOUD_API_SECRET="${trimspace(var.confluent_flink_api_secret)}"
      export CONFLUENT_CMF_URL="https://confluent.cloud"

      # Crear statement usando Confluent CLI
      # Usar printf para evitar problemas de shell con caracteres especiales
      SQL_CONTENT=$(printf '%s\n' '${replace(
        replace(
          local.ddl_data[count.index].statement,
          "$${catalog_name}", var.catalog_name
        ),
        "$${cluster_name}", var.cluster_name
      )}')
      
      confluent flink statement create ddl-statement-${count.index} \
        --sql "$SQL_CONTENT" \
        --compute-pool ${local.compute_pools_map[local.ddl_data[count.index]["flink-compute-pool"]].id} \
        --environment ${var.environment_id} \
        --database ${var.cluster_name}
    EOT
  }

  provisioner "local-exec" {
    when = destroy
    command = <<-EOT
      # Eliminar statement usando Confluent CLI
      # Usar count.index para generar nombre único
      STATEMENT_NAME="ddl-statement-${count.index}"
      confluent flink statement delete $STATEMENT_NAME \
        --force || echo "Statement $STATEMENT_NAME not found or already deleted"
    EOT
  }
  
  # Trigger cuando cambien los datos
  triggers = {
    statement = local.ddl_data[count.index].statement
    compute_pool = local.ddl_data[count.index]["flink-compute-pool"]
  }
}


# -----------------------------------------------------------------------------
# DML Statements (Data Manipulation Language) - Usando local-exec para evitar rest_endpoint
# -----------------------------------------------------------------------------
resource "null_resource" "dml_statements" {
  count = length(local.dml_data)
  
  provisioner "local-exec" {
    command = <<-EOT
      # Configurar credenciales usando archivo de configuración
      mkdir -p ~/.confluent
      cat > ~/.confluent/config << EOF
[api]
api_key = ${trimspace(var.confluent_flink_api_key)}
api_secret = ${trimspace(var.confluent_flink_api_secret)}
url = https://confluent.cloud
EOF

      # Configurar variables de entorno también
      export CONFLUENT_CLOUD_API_KEY="${trimspace(var.confluent_flink_api_key)}"
      export CONFLUENT_CLOUD_API_SECRET="${trimspace(var.confluent_flink_api_secret)}"
      export CONFLUENT_CMF_URL="https://confluent.cloud"

      # Crear statement usando Confluent CLI
      # Usar printf para evitar problemas de shell con caracteres especiales
      SQL_CONTENT=$(printf '%s\n' '${replace(
        replace(
          local.dml_data[count.index].statement,
          "$${catalog_name}", var.catalog_name
        ),
        "$${cluster_name}", var.cluster_name
      )}')
      
      confluent flink statement create dml-statement-${count.index} \
        --sql "$SQL_CONTENT" \
        --compute-pool ${local.compute_pools_map[local.dml_data[count.index]["flink-compute-pool"]].id} \
        --environment ${var.environment_id} \
        --database ${var.cluster_name}
    EOT
  }

  provisioner "local-exec" {
    when = destroy
    command = <<-EOT
      # Eliminar statement usando Confluent CLI
      # Usar count.index para generar nombre único
      STATEMENT_NAME="dml-statement-${count.index}"
      confluent flink statement delete $STATEMENT_NAME \
        --force || echo "Statement $STATEMENT_NAME not found or already deleted"
    EOT
  }
  
  # Trigger cuando cambien los datos
  triggers = {
    statement = local.dml_data[count.index].statement
    compute_pool = local.dml_data[count.index]["flink-compute-pool"]
  }
  
  # Dependencia: DML statements se ejecutan después de DDL
  depends_on = [null_resource.ddl_statements]
}